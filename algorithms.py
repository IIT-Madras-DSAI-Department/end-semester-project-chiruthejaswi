# -*- coding: utf-8 -*-
"""algorithms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iRtNt7lAVVKQe1g_0jmdG7mZ6bMpoDZR
"""

import numpy as np

# Utility

def one_hot(y,num_classes=10):
    Y = np.zeros((len(y),num_classes))
    Y[np.arange(len(y)),y] =1
    return Y


def f1_score(y_true,y_pred):
    f1_list =[]
    for c in range(10):
        tp = np.sum((y_pred == c) & (y_true == c))
        fp = np.sum((y_pred == c) & (y_true != c))
        fn = np.sum((y_pred != c) & (y_true == c))
        if tp == 0:
            f1_list.append(0)
        else:
            f1_list.append(2 *tp/(2 * tp +fp+fn))
    return np.mean(f1_list)

# 1) Softmax Regression (simple linear model, gradient descent)

class SoftmaxRegression:
    def __init__(self,lr=0.05, epochs=50,reg=0.0001):
        self.lr = lr
        self.epochs = epochs
        self.reg = reg

    def softmax(self, z):
        z = z - np.max(z,axis=1, keepdims=True)
        return np.exp(z) /np.sum(np.exp(z),axis=1,keepdims=True)

    def fit(self,X,y):
        n,d =X.shape
        k = 10
        Y= one_hot(y,k)

        self.W = np.zeros((d,k))

        for _ in range(self.epochs):
            logits = X @ self.W
            probs = self.softmax(logits)
            grad = (X.T @ (probs-Y)) /n + self.reg *self.W
            self.W -= self.lr * grad

    def predict(self,X):
        scores= X @ self.W
        return np.argmax(scores, axis=1)

# 2) k-NN Classifier (distance-based learner)

class KNN:
    def __init__(self, k=3):
        self.k = k

    def fit(self, X, y):
        # Just store, since k-NN is lazy learner
        self.X = X
        self.y = y

    def predict(self, X):
        preds = []
        for x in X:
            dist = np.sum((self.X - x)**2, axis=1)
            idx = np.argsort(dist)[:self.k]
            labels = self.y[idx]
            vals, counts = np.unique(labels, return_counts=True)
            preds.append(vals[np.argmax(counts)])
        return np.array(preds)

# 3) Gaussian Naive Bayes (generative class-conditional)


class GaussianNaiveBayes:
    def fit(self, X, y):
        self.means = []
        self.vars = []
        self.priors = []

        for c in range(10):
            Xc = X[y == c]
            self.means.append(np.mean(Xc, axis=0))
            self.vars.append(np.var(Xc, axis=0) + 1e-6)
            self.priors.append(len(Xc) / len(X))

        self.means = np.array(self.means)
        self.vars = np.array(self.vars)
        self.priors = np.array(self.priors)

    def predict(self, X):
        log_probs = []
        for c in range(10):
            mean = self.means[c]
            var = self.vars[c]
            prior = np.log(self.priors[c])
            # Gaussian log-likelihood
            ll = prior - 0.5 * np.sum(np.log(2*np.pi*var) + (X - mean)**2 / var, axis=1)
            log_probs.append(ll)
        log_probs = np.array(log_probs).T
        return np.argmax(log_probs, axis=1)

# 4) K-Means + Nearest-Centroid (clustering-based model)


class KMeansClassifier:
    def __init__(self, k=10, iters=15):
        self.k = k
        self.iters = iters

    def fit(self, X, y):
        n = len(X)
        idx = np.random.choice(n, self.k, replace=False)
        self.centroids = X[idx]

        # KMeans clustering
        for _ in range(self.iters):
            d = np.linalg.norm(X[:, None] - self.centroids[None, :], axis=2)
            labels = np.argmin(d, axis=1)

            for c in range(self.k):
                pts = X[labels == c]
                if len(pts) > 0:
                    self.centroids[c] = np.mean(pts, axis=0)

        # Assign each cluster a "majority" digit
        self.cluster_labels = np.zeros(self.k, dtype=int)
        for c in range(self.k):
            pts = y[labels == c]
            if len(pts) > 0:
                self.cluster_labels[c] = np.argmax(np.bincount(pts))

    def predict(self, X):
        d = np.linalg.norm(X[:, None] - self.centroids[None, :], axis=2)
        idx = np.argmin(d, axis=1)
        return self.cluster_labels[idx]

# 5) Simple Bagging Wrapper

class Bagging:
    def __init__(self, model_class, n_estimators=5):
        self.model_class = model_class
        self.n_estimators = n_estimators
        self.models = []

    def fit(self, X, y):
        n = len(X)
        for _ in range(self.n_estimators):
            bootstrap_idx = np.random.choice(n, n, replace=True)
            model = self.model_class()
            model.fit(X[bootstrap_idx], y[bootstrap_idx])
            self.models.append(model)

    def predict(self, X):
        all_preds = np.array([m.predict(X) for m in self.models])
        final_pred = []
        for i in range(len(X)):
            vals, counts = np.unique(all_preds[:, i], return_counts=True)
            final_pred.append(vals[np.argmax(counts)])
        return np.array(final_pred)