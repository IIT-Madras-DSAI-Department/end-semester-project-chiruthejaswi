# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1own4_NxQy7zYTB2Jy5F98kxhgx7mO4ma

Improved MNIST ensemble using:
 - PCA with whitening
 - k-means++ initialization and cluster-features
 - Random Fourier Features (RFF) for approximate RBF kernel
 - Bagged Softmax models
 - Ridge (closed-form) OVR
 - Nearest Centroid
 - Stacking with closed-form ridge meta-learner on predicted probabilities
"""

# install libraries
import argparse
import time
from typing import Tuple, List

import numpy as np
import pandas as pd
from scipy import linalg

"""**Data Utilities**"""

def load_mnist_from_csv(train_path: str, val_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Load MNIST from CSV files with columns: 'label', '1x1'..'28x28', and possibly 'even'.
    We'll drop 'even' and use 'label' as target and the pixel columns as features.
    Returns: X_train (n_train x 784), y_train, X_val, y_val
    """
    df_train = pd.read_csv(train_path)
    df_val = pd.read_csv(val_path)

    # Drop the derived 'even' column if present
    for df in (df_train, df_val):
        if 'even' in df.columns:
            df.drop(columns=['even'], inplace=True)

    # Ensure label exists
    if 'label' not in df_train.columns or 'label' not in df_val.columns:
        raise ValueError("CSV files must contain a 'label' column.")

    y_train = df_train['label'].values.astype(np.int64)
    y_val = df_val['label'].values.astype(np.int64)

    X_train = df_train.drop(columns=['label']).values.astype(np.float32)
    X_val = df_val.drop(columns=['label']).values.astype(np.float32)

    return X_train, y_train, X_val, y_val

"""**Preprocessing**"""

# Standardisisng
def standardize(X_train: np.ndarray, X_test: np.ndarray):
    mean = X_train.mean(axis=0, keepdims=True)
    std = X_train.std(axis=0, keepdims=True) + 1e-10
    return (X_train - mean) / std, (X_test - mean) / std, mean, std

# PCA_reduction
def pca_reduction(X_train: np.ndarray, X_val: np.ndarray, n_components=100):
    """
    Compute PCA (top components). Return:
     - Xtr_w:whitened projected train (n x k)
     - Xv_w: whitened projected val (m x k)
     - comps:principal components (d x k) (NOT whitened)
     - mean:mean used (1 x d)
     - eigvals:top eigenvalues (k,)
    Whitening is applied to the returned projected features.
    """
    mean = X_train.mean(axis=0,keepdims=True)
    Xc = X_train-mean

    cov = np.dot(Xc.T, Xc)/max(1,(Xc.shape[0]- 1))
    eigvals_all,eigvecs_all = linalg.eigh(cov)
    idx = np.argsort(eigvals_all)[::-1][:n_components]
    eigvals = eigvals_all[idx]
    comps= eigvecs_all[:, idx]  # d x k

    # whitening transform matrix (d x k) = comps / sqrt(eigvals)
    whiten_matrix = comps / np.sqrt(eigvals + 1e-12)

    Xtr_w = (X_train - mean).dot(whiten_matrix)
    Xv_w =(X_val - mean).dot(whiten_matrix)
    return Xtr_w, Xv_w, comps, mean, eigvals


# K-means (with k-means++)

def kmeans_init_plus_plus(X: np.ndarray, k: int, rng: np.random.RandomState):
    n = X.shape[0]
    centers = np.empty((k, X.shape[1]), dtype=X.dtype)
    # pick first center uniformly
    first = rng.randint(0, n)
    centers[0] = X[first]
    # distances squared to nearest center
    closest_d2 = np.sum((X - centers[0]) ** 2, axis=1)
    for i in range(1, k):
        probs = closest_d2 / (closest_d2.sum() + 1e-12)
        idx = rng.choice(n, p=probs)
        centers[i] = X[idx]
        d2 = np.sum((X - centers[i]) ** 2, axis=1)
        closest_d2 = np.minimum(closest_d2, d2)
    return centers


def kmeans(X: np.ndarray, k=120, n_iters=20, seed=0, verbose=False):
    rng = np.random.RandomState(seed)
    n, d = X.shape
    centers = kmeans_init_plus_plus(X, k, rng).astype(np.float32)
    for it in range(n_iters):
        dists = np.sum((X[:, None, :] - centers[None, :, :]) ** 2, axis=2)
        labels = dists.argmin(axis=1)
        new_centers = np.zeros_like(centers)
        counts = np.zeros(k, dtype=np.int32)
        for i in range(k):
            members = X[labels == i]
            if len(members) > 0:
                new_centers[i] = members.mean(axis=0)
                counts[i] = len(members)
            else:
                new_centers[i] = X[rng.randint(0, n)]
                counts[i] = 1
        centers = new_centers
        if verbose:
            inertia = np.sum(np.min(dists, axis=1))
            print(f"kmeans iter {it+1}/{n_iters} inertia={inertia:.2f}")
    return centers


def kmeans_features(X: np.ndarray, centers: np.ndarray):
    # distances to centers -> similarity features (soft)
    dists = np.linalg.norm(X[:, None, :] - centers[None, :, :], axis=2)
    sim = 1.0 / (dists + 1e-8)
    sim = sim / sim.sum(axis=1, keepdims=True)
    return sim.astype(np.float32)

# Random Fourier Features

def rff_features(X: np.ndarray, n_features=500, sigma=5.0, seed=0):
    """
    Returns Z = cos(X W + b) / sqrt(n_features)
    Also returns (W, b) for transforming other sets.
    W shape: (d, n_features)
    b shape: (n_features,)
    """
    rng = np.random.RandomState(seed)
    n, d = X.shape
    W = rng.normal(scale=1.0 / sigma, size=(d, n_features)).astype(np.float32)
    b = rng.uniform(0, 2 * np.pi, size=n_features).astype(np.float32)
    Z = np.cos(X.dot(W) + b) / np.sqrt(n_features)
    return Z.astype(np.float32), (W, b)


def rff_transform(X: np.ndarray, params):
    W, b = params
    Z = np.cos(X.dot(W) + b) / np.sqrt(W.shape[1])
    return Z.astype(np.float32)

"""**Models**"""

# Models

def onehot(y, n_classes=None):
    if n_classes is None:
        n_classes = int(y.max()) + 1
    Y = np.zeros((y.shape[0], n_classes), dtype=np.float32)
    Y[np.arange(y.shape[0]), y] = 1.0
    return Y


class RidgeOVR:
    def __init__(self, reg=1e-3):
        self.reg = reg
        self.W = None
        self.b = None

    def fit(self, X, y):
        n, d = X.shape
        C = int(y.max()) + 1
        Y = onehot(y, C)
        Xb = np.hstack([X, np.ones((n, 1), dtype=X.dtype)])
        A = Xb.T.dot(Xb) + self.reg * np.eye(d + 1)
        B = Xb.T.dot(Y)
        W = linalg.solve(A, B)
        self.W = W[:-1]
        self.b = W[-1]

    def predict_proba(self, X):
        logits = X.dot(self.W) + self.b
        logits = logits - logits.max(axis=1, keepdims=True)
        e = np.exp(logits)
        P = e / e.sum(axis=1, keepdims=True)
        return P

    def predict(self, X):
        return np.argmax(self.predict_proba(X), axis=1)


class Softmax:
    def __init__(self, lr=0.5, reg=1e-4, epochs=30, batch_size=256, verbose=False):
        self.lr = lr
        self.reg = reg
        self.epochs = epochs
        self.batch_size = batch_size
        self.verbose = verbose
        self.W = None
        self.b = None

    def _loss_and_grad(self, X, Y):
        n = X.shape[0]
        logits = X.dot(self.W) + self.b
        logits = logits - logits.max(axis=1, keepdims=True)
        exp = np.exp(logits)
        probs = exp / exp.sum(axis=1, keepdims=True)
        loss = -np.sum(Y * np.log(probs + 1e-12)) / n
        loss += 0.5 * self.reg * np.sum(self.W * self.W)
        gradW = X.T.dot(probs - Y) / n + self.reg * self.W
        gradb = probs.mean(axis=0) - Y.mean(axis=0)
        return loss, gradW, gradb

    def fit(self, X, y):
        n, d = X.shape
        C = int(y.max()) + 1
        Y = onehot(y, C)
        rng = np.random.RandomState(0)
        self.W = 0.01 * rng.randn(d, C).astype(np.float32)
        self.b = np.zeros(C, dtype=np.float32)
        idx = np.arange(n)
        for epoch in range(self.epochs):
            rng.shuffle(idx)
            for i in range(0, n, self.batch_size):
                batch = idx[i:i + self.batch_size]
                Xb = X[batch]
                Yb = Y[batch]
                _, gW, gb = self._loss_and_grad(Xb, Yb)
                self.W -= self.lr * gW
                self.b -= self.lr * gb
            if self.verbose and (epoch % 5 == 0 or epoch == self.epochs - 1):
                loss, _, _ = self._loss_and_grad(X, Y)
                print(f"Softmax epoch {epoch+1}/{self.epochs}, loss={loss:.4f}")

    def predict_proba(self, X):
        logits = X.dot(self.W) + self.b
        logits = logits - logits.max(axis=1, keepdims=True)
        e = np.exp(logits)
        P = e / e.sum(axis=1, keepdims=True)
        return P

    def predict(self, X):
        return np.argmax(self.predict_proba(X), axis=1)


class NearestCentroid:
    def __init__(self):
        self.centers = None

    def fit(self, X, y):
        C = int(y.max()) + 1
        d = X.shape[1]
        centers = np.zeros((C, d), dtype=np.float32)
        for c in range(C):
            members = X[y == c]
            if len(members) == 0:
                centers[c] = np.zeros(d, dtype=np.float32)
            else:
                centers[c] = members.mean(axis=0)
        self.centers = centers

    def predict(self, X):
        dists = np.linalg.norm(X[:, None, :] - self.centers[None, :, :], axis=2)
        return dists.argmin(axis=1)

    def predict_proba(self, X):
        dists = np.linalg.norm(X[:, None, :] - self.centers[None, :, :], axis=2)
        sim = 1.0 / (dists + 1e-8)
        return sim / sim.sum(axis=1, keepdims=True)

"""**Anomaly detection**"""

def pca_reconstruction_error(X, comps, X_mean):
    """
    comps: d x k (principal components)
    X_mean: (1 x d)
    Returns reconstruction MSE per sample using the k components.
    """
    proj = (X - X_mean).dot(comps)  # n x k
    recon = proj.dot(comps.T) + X_mean
    err = np.mean((X - recon) ** 2, axis=1)
    return err

"""**Metrics**"""

def f1_scores(y_true, y_pred):
    K = int(max(y_true.max(), y_pred.max())) + 1
    f1s = []
    for c in range(K):
        tp = np.sum((y_pred == c) & (y_true == c))
        fp = np.sum((y_pred == c) & (y_true != c))
        fn = np.sum((y_pred != c) & (y_true == c))
        prec = tp / (tp + fp + 1e-12)
        rec = tp / (tp + fn + 1e-12)
        f1 = 2 * prec * rec / (prec + rec + 1e-12)
        f1s.append(f1)
    macro = float(np.mean(f1s))
    acc = float(np.mean(y_true == y_pred))
    return {'per_class': f1s, 'macro': macro, 'accuracy': acc}

"""**Stacking**"""

def stack_train_closed_form(preds_list: List[np.ndarray], y_val: np.ndarray, lam=1e-3):
    """
    preds_list: list of (n_samples x n_classes) arrays (probabilities) for training set
    Returns meta_W: (total_meta_features x C) weight matrix (no bias)
    We'll add small l2 reg (lam).
    """
    Xmeta = np.hstack(preds_list)
    C = int(y_val.max()) + 1
    Y = onehot(y_val, C)
    A = Xmeta.T.dot(Xmeta) + lam * np.eye(Xmeta.shape[1])
    B = Xmeta.T.dot(Y)
    W = linalg.solve(A, B)
    return W


def stack_predict_closed_form(meta_W: np.ndarray, preds_list: List[np.ndarray]):
    Xmeta = np.hstack(preds_list)
    logits = Xmeta.dot(meta_W)
    return logits  # not softmaxed; we will argmax logits

"""**Bagging Helper**"""

def train_bagged_softmax(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray, args, n_models=5, seed=1):
    bag_models = []
    rng = np.random.RandomState(seed)
    n = X.shape[0]
    prob = sample_weights / (sample_weights.sum() + 1e-12)
    sample_size = min(n, args.soft_sample_size)
    for b in range(n_models):
        idx = rng.choice(n, size=sample_size, replace=True, p=prob)
        model = Softmax(lr=args.soft_lr, reg=args.soft_reg, epochs=args.soft_epochs, batch_size=args.soft_batch, verbose=args.verbose)
        model.fit(X[idx], y[idx])
        bag_models.append(model)
    return bag_models

# Main pipeline

def main(args):
    t0 = time.time()
    X_train, y_train, X_val, y_val = load_mnist_from_csv(args.train, args.val)
    print('Loaded CSV MNIST:', X_train.shape, y_train.shape, X_val.shape, y_val.shape)

    # normalize pixels to [0,1]
    X_train = X_train / 255.0
    X_val = X_val / 255.0

    # standardize
    Xtr_s, Xte_s, X_mean_raw, X_std_raw = standardize(X_train, X_val)

    # PCA with whitening
    Xtr_pca, Xte_pca, comps, pca_mean, eigvals = pca_reduction(Xtr_s, Xte_s, n_components=args.pca_components)

    # k-means on PCA features (whitened PCA features)
    centers = kmeans(Xtr_pca, k=args.kmeans_k, n_iters=args.kmeans_iters, seed=args.kmeans_seed, verbose=args.verbose)
    Xtr_kf = kmeans_features(Xtr_pca, centers)
    Xte_kf = kmeans_features(Xte_pca, centers)

    # final features: PCA-whitened + kmeans features
    Xtr_final = np.hstack([Xtr_pca, Xtr_kf]).astype(np.float32)
    Xte_final = np.hstack([Xte_pca, Xte_kf]).astype(np.float32)
    print('Final feature shapes:', Xtr_final.shape, Xte_final.shape)

    # anomaly detection -> simple sample weights via PCA reconstruction error (use non-whitened comps)
    # Note: recon err computed on standardized features
    recon_err = pca_reconstruction_error(Xtr_s, comps, Xtr_s.mean(axis=0, keepdims=True))
    weights = 1.0 / (1.0 + recon_err)
    weights = weights / (weights.mean() + 1e-12)

    # Train base models
    models = {}

    print('Training Ridge OVR...')
    ridge = RidgeOVR(reg=args.ridge_reg)
    ridge.fit(Xtr_final, y_train)
    models['ridge'] = ridge

    print('Training Softmax (global)...')
    soft = Softmax(lr=args.soft_lr, reg=args.soft_reg, epochs=args.soft_epochs, batch_size=args.soft_batch, verbose=args.verbose)
    # use weighted sampling for initial softmax
    rng = np.random.RandomState(0)
    n = Xtr_final.shape[0]
    sample_prob = weights / (weights.sum() + 1e-12)
    sample_size = min(n, args.soft_sample_size)
    idxs = rng.choice(n, size=sample_size, replace=True, p=sample_prob)
    soft.fit(Xtr_final[idxs], y_train[idxs])
    models['softmax'] = soft

    print('Training Nearest Centroid...')
    nc = NearestCentroid()
    nc.fit(Xtr_final, y_train)
    models['nc'] = nc

    # Bagged Softmax models
    print(f'Training {args.n_bag_models} bagged Softmax models...')
    bag_models = train_bagged_softmax(Xtr_final, y_train, weights, args, n_models=args.n_bag_models, seed=args.kmeans_seed + 100)
    for i, m in enumerate(bag_models):
        models[f"soft_bag_{i}"] = m

    # Build RFF features from final features (so RFF sees both PCA and kmeans signals)
    print("Building RFF kernel features and training RFF-Softmax...")
    Ztr, rff_params = rff_features(Xtr_final, n_features=args.rff_features, sigma=args.rff_sigma, seed=args.rff_seed)
    Zte = rff_transform(Xte_final, rff_params)
    rff_softmax = Softmax(lr=args.rff_lr, reg=args.rff_reg, epochs=args.rff_epochs, batch_size=args.rff_batch, verbose=args.verbose)
    rff_softmax.fit(Ztr, y_train)
    models['rff'] = rff_softmax

    # Evaluate base models on validation
    print('\nBase model results on validation:')
    preds_val = []
    for name, m in models.items():
        # handle rff specially (transform features)
        if name == 'rff':
            P = m.predict_proba(Zte)
        else:
            P = m.predict_proba(Xte_final)
        preds_val.append(P)
        y_pred = np.argmax(P, axis=1)
        s = f1_scores(y_val, y_pred)
        print(f"{name:12s}  macro-F1={s['macro']:.4f}  acc={s['accuracy']:.4f}")

    # Train meta-learner (closed-form ridge) on TRAIN predictions
    print('\nTraining meta-learner (closed-form ridge) on train predictions...')
    preds_train = []
    for name, m in models.items():
        if name == 'rff':
            P = m.predict_proba(Ztr)
        else:
            P = m.predict_proba(Xtr_final)
        preds_train.append(P)

    meta_W = stack_train_closed_form(preds_train, y_train, lam=args.meta_reg)

    # Meta predict on validation
    meta_logits = stack_predict_closed_form(meta_W, preds_val)
    y_meta = np.argmax(meta_logits, axis=1)
    s_meta = f1_scores(y_val, y_meta)
    print(f"\nStacked model  macro-F1={s_meta['macro']:.4f}  acc={s_meta['accuracy']:.4f}")

    # Tabulate
    print('\nResults table:')
    print('{:<15s}{:>12s}{:>12s}'.format('Model', 'Macro-F1', 'Accuracy'))
    for name, m in models.items():
        if name == 'rff':
            P = m.predict_proba(Zte)
        else:
            P = m.predict_proba(Xte_final)
        y_pred = np.argmax(P, axis=1)
        s = f1_scores(y_val, y_pred)
        print('{:<15s}{:12.4f}{:12.4f}'.format(name, s['macro'], s['accuracy']))
    print('{:<15s}{:12.4f}{:12.4f}'.format('stacked', s_meta['macro'], s_meta['accuracy']))

    t_total = time.time() - t0
    print(f"\nTotal runtime: {t_total:.1f} seconds")
    if t_total > 300:
        print('Warning: runtime > 5 minutes. Consider reducing PCA components, kmeans k, or training epochs.')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--train', type=str, default='/content/MNIST_train.csv', help='Path to train CSV')
    parser.add_argument('--val', type=str, default='/content/MNIST_validation.csv', help='Path to validation CSV')
    parser.add_argument('--pca_components', type=int, default=60)
    parser.add_argument('--kmeans_k', type=int, default=50)
    parser.add_argument('--kmeans_iters', type=int, default=12)
    parser.add_argument('--ridge_reg', type=float, default=1e-2)
    parser.add_argument('--soft_lr', type=float, default=0.5)
    parser.add_argument('--soft_reg', type=float, default=1e-3)
    parser.add_argument('--soft_epochs', type=int, default=40)
    parser.add_argument('--soft_batch', type=int, default=512)
    parser.add_argument('--soft_sample_size', type=int, default=20000)
    parser.add_argument('--kmeans_seed', type=int, default=0)
    parser.add_argument('--verbose', action='store_true')

    # RFF params
    parser.add_argument('--rff_features', type=int, default=600)
    parser.add_argument('--rff_sigma', type=float, default=4.5)
    parser.add_argument('--rff_seed', type=int, default=42)
    parser.add_argument('--rff_lr', type=float, default=0.3)
    parser.add_argument('--rff_reg', type=float, default=1e-3)
    parser.add_argument('--rff_epochs', type=int, default=50)
    parser.add_argument('--rff_batch', type=int, default=512)

    # Bagging
    parser.add_argument('--n_bag_models', type=int, default=10)

    # Meta
    parser.add_argument('--meta_reg', type=float, default=1e-3)

    args = parser.parse_args([])
    main(args)

